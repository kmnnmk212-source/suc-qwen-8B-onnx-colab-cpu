# -*- coding: utf-8 -*-
"""suc_qwen8b_onnx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18jeDoLz0-xWRf_-YYiG9Ng1GR6cJBsOC
"""







!npm i @huggingface/transformers

# Commented out IPython magic to ensure Python compatibility.
# 
# %%writefile 1.js
# import { pipeline, TextStreamer } from "@huggingface/transformers";
# 
# // Create a text generation pipeline
# const generator = await pipeline(
#   "text-generation",
#   "onnx-community/gemma-3-270m-it-ONNX",
#   { dtype: "fp32" },
# );
# 
# // Define the list of messages
# const messages = [
#   { role: "system", content: "You are a helpful assistant." },
#   { role: "user", content: "Write a poem about machine learning." },
# ];
# 
# // Generate a response
# const output = await generator(messages, {
#   max_new_tokens: 512,
#   do_sample: false,
#   streamer: new TextStreamer(generator.tokenizer, {
#     skip_prompt: true,
#     skip_special_tokens: true,
#     // callback_function: (text) => { /* Optional callback function */ },
#   }),
# });
# console.log(output[0].generated_text.at(-1).content);
#

!node 1.js

# Commented out IPython magic to ensure Python compatibility.
# 
# 
# %%writefile 3.js
# import { pipeline, TextStreamer } from "@huggingface/transformers";
# 
# // Create a text generation pipeline
# const generator = await pipeline(
#   "text-generation",
#   "onnx-community/Qwen3-0.6B-ONNX",
#   { dtype: "int8" },
# );
# 
# // Define the list of messages
# const messages = [
#   { role: "system", content: "You are a helpful assistant." },
#   { role: "user", content: "Write a poem about machine learning." },
# ];
# 
# // Generate a response
# const output = await generator(messages, {
#   max_new_tokens: 512,
#   do_sample: false,
#   streamer: new TextStreamer(generator.tokenizer, {
#     skip_prompt: true,
#     skip_special_tokens: true,
#     // callback_function: (text) => { /* Optional callback function */ },
#   }),
# });
# console.log(output[0].generated_text.at(-1).content);
#

!node 3.js

model_uint8.onnx

# Commented out IPython magic to ensure Python compatibility.
# 
# %%writefile 4.js
# 
# import { pipeline, TextStreamer } from "@huggingface/transformers";
# 
# // Create a text generation pipeline
# const generator = await pipeline(
#   "text-generation",
#   "onnx-community/Qwen3-0.6B-ONNX",
#   { dtype: "uint8" },
# );
# 
# // Define the list of messages
# const messages = [
#   { role: "system", content: "You are a helpful assistant." },
#   { role: "user", content: "Write me a poem about Machine Learning." },
# ];
# 
# // Generate a response
# const output = await generator(messages, {
#   max_new_tokens: 512,
#   do_sample: false,
#   streamer: new TextStreamer(generator.tokenizer, { skip_prompt: true, skip_special_tokens: true}),
# });
# console.log(output[0].generated_text.at(-1).content);
#

!node 4.js

import { pipeline } from '@huggingface/transformers';

// Allocate pipeline
const pipe = await pipeline('text-generation', 'Xenova/Qwen1.5-1.8B-Chat');

# Commented out IPython magic to ensure Python compatibility.
# %%writefile 5.js
# 
# import { pipeline, TextStreamer } from "@huggingface/transformers";
# 
# // Allocate pipeline
# const pipe = await pipeline(
#   "text-generation",
#   "Xenova/Qwen1.5-1.8B-Chat",
#   //{ dtype: "uint8" }
# );
# // Define the list of messages
# const messages = [
#   { role: "system", content: "You are a helpful assistant." },
#   { role: "user", content: "Write me a poem about Machine Learning." },
# ];
# 
# // Generate a response
# const output = await pipe(messages, {
#   max_new_tokens: 512,
#   do_sample: false,
#   streamer: new TextStreamer(pipe.tokenizer, { skip_prompt: true, skip_special_tokens: true}),
# });
# console.log(output[0].generated_text.at(-1).content);

!node 5.js

!node 5.js

onnx-community/DeepSeek-R1-Distill-Qwen-1.5B-ONNX

# Commented out IPython magic to ensure Python compatibility.
# %%writefile 6.js
# import { pipeline } from '@huggingface/transformers';
# 
# // Allocate pipeline
# const pipe = await pipeline('text-generation', 'Xenova/Qwen1.5-1.8B-Chat');

!node 6.js

# Commented out IPython magic to ensure Python compatibility.
# %%writefile 6.js
# 
# import { pipeline, TextStreamer } from "@huggingface/transformers";
# 
# // Allocate pipeline
# const pipe = await pipeline(
#   "text-generation",
#   "from transformers import AutoTokenizer
# 
# from optimum.onnxruntime import ORTModelForCausalLM
# 
# import torch
# 
# tokenizer = AutoTokenizer.from_pretrained("optimum/gpt2")
# 
# model = ORTModelForCausalLM.from_pretrained("optimum/gpt2")
# 
# inputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")
# 
# gen_tokens = model.generate(**inputs,do_sample=True,temperature=0.9, min_length=20,max_length=20)
# 
# tokenizer.batch_decode(gen_tokens)",
#   { dtype: "fp32" }
# );
# // Define the list of messages
# const messages = [
#   { role: "system", content: "You are a helpful assistant." },
#   { role: "user", content: "Write me a poem about Machine Learning." },
# ];
# 
# // Generate a response
# const output = await pipe(messages, {
#   max_new_tokens: 512,
#   do_sample: false,
#   streamer: new TextStreamer(pipe.tokenizer, { skip_prompt: true, skip_special_tokens: true}),
# });
# console.log(output[0].generated_text.at(-1).content);

!node 6.js

# Commented out IPython magic to ensure Python compatibility.
# %%writefile 6.js
# import { pipeline } from '@huggingface/transformers';
# 
# // Create text-generation pipeline
# const generator = await pipeline('text-generation', 'Xenova/Qwen1.5-0.5B-Chat');
# 
# // Define the prompt and list of messages
# const prompt = 'Give me a short introduction to large language model.'
# const messages = [
#     { role: 'system', content: 'You are a helpful assistant.' },
#     { role: 'user', content: prompt }
# ]
# 
# // Apply chat template
# const text = generator.tokenizer.apply_chat_template(messages, {
#     tokenize: false,
#     add_generation_prompt: true,
# });
# 
# // Generate text
# const output = await generator(text, {
#     max_new_tokens: 128,
#     do_sample: false,
#     return_full_text: false,
# });
# console.log(output[0].generated_text);
# // 'A large language model is a type of artificial intelligence system that can generate text based on the input provided by users, such as books, articles, or websites. It uses advanced algorithms and techniques to learn from vast amounts of data and improve its performance over time through machine learning and natural language processing (NLP). Large language models have become increasingly popular in recent years due to their ability to handle complex tasks such as generating human-like text quickly and accurately. They have also been used in various fields such as customer service chatbots, virtual assistants, and search engines for information retrieval purposes.'
#

!node 6.js

# Commented out IPython magic to ensure Python compatibility.
# 
# %%writefile 6.js
# import { pipeline } from '@huggingface/transformers';
# 
# // Create text-generation pipeline
# const generator = await pipeline('text-generation', 'Xenova/Qwen1.5-1.8B-Chat');
# 
# // Define the prompt and list of messages
# const prompt = 'Give me a short introduction to large language model.'
# const messages = [
#     { role: 'system', content: 'You are a helpful assistant.' },
#     { role: 'user', content: prompt }
# ]
# 
# // Apply chat template
# const text = generator.tokenizer.apply_chat_template(messages, {
#     tokenize: false,
#     add_generation_prompt: true,
# });
# 
# // Generate text
# const output = await generator(text, {
#     max_new_tokens: 128,
#     do_sample: false,
#     return_full_text: false,
# });
# console.log(output[0].generated_text);
# // 'A large language model is a type of artificial intelligence system that can generate text based on the input provided by users, such as books, articles, or websites. It uses advanced algorithms and techniques to learn from vast amounts of data and improve its performance over time through machine learning and natural language processing (NLP). Large language models have become increasingly popular in recent years due to their ability to handle complex tasks such as generating human-like text quickly and accurately. They have also been used in various fields such as customer service chatbots, virtual assistants, and search engines for information retrieval purposes.'
#

!node 6.js

!pip install "optimum-onnx[onnxruntime]"

from transformers import AutoTokenizer

from optimum.onnxruntime import ORTModelForCausalLM

import torch

tokenizer = AutoTokenizer.from_pretrained("Xenova/Qwen1.5-1.8B-Chat")

model = ORTModelForCausalLM.from_pretrained("Xenova/Qwen1.5-1.8B-Chat")

inputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")

gen_tokens = model.generate(**inputs,do_sample=True,temperature=0.9, min_length=20,max_length=20)

tokenizer.batch_decode(gen_tokens)

print(gen_tokens)

import torch
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Xenova/Qwen1.5-1.8B-Chat")

# Explicitly specify the ONNX file name as suggested by the warning messages
# The model repository has 'onnx/decoder_model_merged.onnx' as the main file.
model = ORTModelForCausalLM.from_pretrained(
    "Xenova/Qwen1.5-1.8B-Chat",
    file_name="decoder_model_merged.onnx", # <-- Add this argument
    subfolder="onnx" # <-- Add this argument (since the file is in an 'onnx' subfolder)
)

inputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")

# If the issue persists, you might need to try the quantized version as well,
# as the merged (non-quantized) file is very large (7.35G) and might be failing
# due to memory or disk issues during load/copy, or it might be a corrupt download.
# For now, try the fix above.
gen_tokens = model.generate(**inputs,do_sample=True,temperature=0.9, min_length=20,max_length=20)

print(tokenizer.batch_decode(gen_tokens))



"""https://huggingface.co/Xenova/Qwen1.5-1.8B-Chat/tree/main/onnx"""

import torch
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Xenova/Qwen1.5-1.8B-Chat")

# 1. Try loading the quantized version instead.
#    This model is often smaller and might not rely on the external .onnx_data file,
#    thus avoiding the file naming issue.
model = ORTModelForCausalLM.from_pretrained(
    "Xenova/Qwen1.5-1.8B-Chat",
    file_name="decoder_model_merged_quantized.onnx", # <-- CHANGE: Use quantized file
    subfolder="onnx"
)

inputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")

gen_tokens = model.generate(**inputs,do_sample=True,temperature=0.9, min_length=20,max_length=20)

print(tokenizer.batch_decode(gen_tokens))

import torch
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Xenova/Qwen1.5-1.8B-Chat")

# 1. Try loading the quantized version instead.
#    This model is often smaller and might not rely on the external .onnx_data file,
#    thus avoiding the file naming issue.
model = ORTModelForCausalLM.from_pretrained(
    "Xenova/Qwen1.5-1.8B-Chat",
    file_name="decoder_model_merged_quantized.onnx", # <-- CHANGE: Use quantized file
    subfolder="onnx"
)

inputs = tokenizer("My name is Arthur and I live in", return_tensors="pt")

gen_tokens = model.generate(**inputs,do_sample=True,temperature=0.9, min_length=20,max_length=20)

print(tokenizer.batch_decode(gen_tokens))




onnx/decoder_model_merged_quantized.onnx: 100%
 1.87G/1.87G [00:40<00:00, 69.6MB/s]

Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.

['My name is Arthur and I live in the city of Cambridge, England. Cambridge is a great place']

"""https://huggingface.co/Xenova/Qwen1.5-1.8B-Chat/discussions/2

https://huggingface.co/Xenova/models?search=qwen
"""

import torch
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("Xenova/Qwen1.5-1.8B-Chat")

# 1. Try loading the quantized version instead.
#    This model is often smaller and might not rely on the external .onnx_data file,
#    thus avoiding the file naming issue.
model = ORTModelForCausalLM.from_pretrained(
    "Xenova/Qwen1.5-1.8B-Chat",
    file_name="decoder_model_merged_quantized.onnx", # <-- CHANGE: Use quantized file
    subfolder="onnx"
)

inputs = tokenizer("qwen model is", return_tensors="pt")

gen_tokens = model.generate(**inputs,do_sample=True,temperature=0.9, min_length=20,max_length=20)

print(tokenizer.batch_decode(gen_tokens))

